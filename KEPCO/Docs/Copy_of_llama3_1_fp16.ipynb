{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "712Em134t9B-"
      },
      "source": [
        "# LLama3.1 8B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oHH7CscCt9CA",
        "outputId": "f3574620-242f-463d-ecde-aeec3d23688c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'unsloth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d9885e781dad>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhuggingface_hub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhuggingface_hub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"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\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unsloth'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "import huggingface_hub\n",
        "huggingface_hub.login(\"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\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB22P2omt9CC"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = False\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    cache_dir=\"/workspace/nmquy/hf_cache\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZzAUos7t9CD"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_QJaKuZt9CE"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmlW1vnut9CF"
      },
      "source": [
        "### Convert JSON to HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gox6TE5qt9CG"
      },
      "outputs": [],
      "source": [
        "instruction = \"Chuyển dictionary python sau từ JSON sang bảng HTML có header và title cột. Chỉ sử dụng mã HTML:\"\n",
        "input_data = '''\n",
        "{'resturant employees': [{'name': 'Shyam', 'email': 'shyamjaiswal@gmail.com'}, {'name': 'Bob', 'email': 'bob32@gmail.com'}, {'name': 'Jai', 'email': 'jai87@gmail.com'}]}\n",
        "'''\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        instruction,  # instruction\n",
        "        input_data,   # input\n",
        "        \"\"            # output\n",
        "    )\n",
        "], return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
        "html_output = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
        "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
        "print(response_part)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMYQKv1Jt9CH"
      },
      "source": [
        "### Few-shot rewrite question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWMVENw_t9CH"
      },
      "outputs": [],
      "source": [
        "instruction = \"You are able to reason from previous conversation and the recent question, to come up with a rewrite of the question which is concise but with enough information that people without knowledge of previous conversation can understand the question:\"\n",
        "input_data = '''\n",
        "## Previous conversation\n",
        "user: What is BERT?\n",
        "assistant: BERT stands for \"Bidirectional Encoder Representations from Transformers.\" It is a natural language processing (NLP) model developed by Google.\n",
        "user: What data was used for its training?\n",
        "assistant: The BERT (Bidirectional Encoder Representations from Transformers) model was trained on a large corpus of publicly available text from the internet. It was trained on a combination of books, articles, websites, and other sources to learn the language patterns and relationships between words.\n",
        "## Question\n",
        "user: What NLP tasks can it perform well?\n",
        "## Rewritten question\n",
        "'''\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        instruction,  # instruction\n",
        "        input_data,   # input\n",
        "        \"\"            # output\n",
        "    )\n",
        "], return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
        "html_output = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
        "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
        "print(response_part)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTygbl-At9CH"
      },
      "source": [
        "### Make an instruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqomoMlht9CI"
      },
      "outputs": [],
      "source": [
        "instruction = \"\"\"Bạn sẽ được cung cấp văn bản. Nếu nó chứa một chuỗi các hướng dẫn, viết lại các hướng dẫn đó theo định dạng sau:\n",
        "Bước 1 - ...\n",
        "Bước 2 -…\n",
        "…\n",
        "Bước N -…\n",
        "\n",
        "Nếu văn bản không chứa một chuỗi hướng dẫn, sau đó chỉ cần viết \"No steps provided.\\\"\"\"\"\n",
        "input_data = '''\n",
        "Làm một tách trà thật dễ dàng! Đầu tiên, bạn cần lấy một số  nước sôi. Trong khi điều đó đang xảy ra,\n",
        "lấy một chiếc cốc và đặt một túi trà vào đó. Một khi nước đã đủ nóng, chỉ cần đổ nó lên túi trà.\n",
        "Hãy để nó ngồi một chút để trà có thể ngâm. Sau một vài phút, lấy túi trà ra. Nếu bạn\n",
        "như, bạn có thể thêm một ít đường hoặc sữa cho vừa ăn. Và thế là xong! Bạn đã có cho mình một món ngon tách trà để thưởng thức.\n",
        "'''\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        instruction,  # instruction\n",
        "        input_data,   # input\n",
        "        \"\"            # output\n",
        "    )\n",
        "], return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
        "html_output = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
        "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
        "print(response_part)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT5bCzDut9CI"
      },
      "source": [
        "### Follow many instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SHfx60ct9CI"
      },
      "outputs": [],
      "source": [
        "instruction = \"\"\"Thực hiện các hành động sau:\n",
        "1 - Tóm tắt đoạn văn bản sau.\n",
        "2 - Dịch tóm tắt sang tiếng Anh.\n",
        "3 - Xuất ra một đối tượng json chứa các khóa sau: Vietnamese_summary, English_summary.\n",
        "Tách các câu trả lời của bạn bằng dấu ngắt dòng.\"\"\"\n",
        "\n",
        "input_data = \"\"\"\n",
        "Khoảng 15h, nhà kho chứa đồ gỗ Công ty TNHH Vượng Hà ở xã Cương Sơn, huyện Lục Nam bốc cháy. Một người đàn ông từ bên trong chạy ra ngoài hô hoán nhờ hỗ trợ.\n",
        "\n",
        "Tuy nhiên do vật liệu dễ cháy nên chỉ trong ít phút, ngọn lửa đã bùng lên bất chấp nỗ lực chữa cháy của người dân. Khoảng 15 phút sau, ngọn lửa đã lan ra toàn bộ nhà xưởng, cột cói nghi ngút bốc cao hàng chục mét.\n",
        "Phòng Cảnh sát phòng cháy, chữa cháy và cứu nạn cứu hộ, Công an huyện Lục Nam đã huy động 40 chiến sĩ cùng ba xe chữa cháy đến hiện trường. Sau khoảng 40 phút, ngọn lửa được dập tắt nhưng hầu hết tài sản bên trong bị thiêu rụi.\n",
        "\n",
        "Ông Nguyễn Mạnh Hưng, Chủ tịch UBND xã Cương Sơn, cho biết vụ cháy không gây thiệt hại về người. Tuy nhiên, xưởng gỗ rộng khoảng 400 m2 nằm sát đường tỉnh 293 đã bị thiêu rụi. Toàn bộ phần mái tôn và cấu kiện bên trong sập đổ, tường tôn bị uốn cong, biến dạng.\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        instruction,  # instruction\n",
        "        input_data,   # input\n",
        "        \"\"            # output\n",
        "    )\n",
        "], return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=512, use_cache=True)\n",
        "html_output = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
        "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
        "print(response_part)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3yqCmzUt9CJ"
      },
      "source": [
        "### Logic check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRw0KhJSt9CJ"
      },
      "outputs": [],
      "source": [
        "instruction = \"\"\"Xác định xem cách giải quyết của học sinh có đúng hay không.\"\"\"\n",
        "\n",
        "input_data = \"\"\"\n",
        "Câu hỏi: Tôi đang lên kế hoạch xây dựng một tòa nhà văn phòng và cần tính toán chi phí cho năm đầu tiên hoạt động.\n",
        "\n",
        "Giá đất là 200 USD/m2.\n",
        "Tôi có thể mua vật liệu xây dựng với giá 300 USD/m2.\n",
        "Chi phí bảo trì hằng năm là 50 nghìn USD cho toàn bộ tòa nhà và thêm 15 USD/m2. Tổng chi phí cho năm đầu tiên hoạt động là bao nhiêu, như một hàm của diện tích tòa nhà tính bằng mét vuông.\n",
        "Giải pháp của học sinh: Gọi y là diện tích của tòa nhà tính bằng mét vuông.\n",
        "\n",
        "Chi phí:\n",
        "Giá đất: 200y USD\n",
        "Giá vật liệu xây dựng: 300y USD\n",
        "Chi phí bảo trì: 50.000+15y USD\n",
        "Tổng chi phí:\n",
        "200y+300y+50.000+15y=515y+50.000 (USD)\n",
        "Tổng chi phí cho năm đầu tiên sẽ là : 515y+50.000 USD, với y là diện tích của tòa nhà.\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        instruction,  # instruction\n",
        "        input_data,   # input\n",
        "        \"\"            # output\n",
        "    )\n",
        "], return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=512, use_cache=True)\n",
        "html_output = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
        "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
        "print(response_part)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOuxc_v2t9CJ"
      },
      "source": [
        "### Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhmJ8GYkt9CJ"
      },
      "outputs": [],
      "source": [
        "instruction = \"\"\"Nhiệm vụ của bạn là tạo một bản tóm tắt ngắn về đánh giá sản phẩm từ một trang web thương mại điện tử.\n",
        "Tóm tắt đánh giá dưới đây tối đa 30 từ.\"\"\"\n",
        "\n",
        "input_data = \"\"\"\n",
        "Chiếc điện thoại thông minh nhận được nhiều đánh giá tích cực từ khách hàng trên trang web thương mại điện tử. Họ khen ngợi hiệu năng mạnh mẽ, tốc độ xử lý nhanh và mượt mà, cùng với thời lượng pin ấn tượng, có thể sử dụng cả ngày mà không cần sạc. Màn hình sắc nét và tính năng bảo mật vân tay tiện lợi cũng là những điểm cộng. Tuy nhiên, một số khách hàng cho rằng camera chưa thực sự tốt trong điều kiện thiếu sáng, thiết kế của điện thoại hơi cồng kềnh, và loa ngoài không đủ lớn. Với mức giá hợp lý, sản phẩm này vẫn được đánh giá cao về chất lượng tổng thể.\"\"\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        instruction,  # instruction\n",
        "        input_data,   # input\n",
        "        \"\"            # output\n",
        "    )\n",
        "], return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n",
        "html_output = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
        "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
        "print(response_part)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZFRY3V9t9CK"
      },
      "source": [
        "### Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vy-Em9wYt9CK"
      },
      "outputs": [],
      "source": [
        "instruction = \"\"\"Cảm nhận của bài đánh giá sản phẩm sau đây là gì?\"\"\"\n",
        "\n",
        "input_data = \"\"\"\n",
        "Hiệu năng tuyệt vời, điện thoại chạy mượt mà và nhanh chóng khi sử dụng các ứng dụng nặng.\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        instruction,  # instruction\n",
        "        input_data,   # input\n",
        "        \"\"            # output\n",
        "    )\n",
        "], return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n",
        "html_output = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
        "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
        "print(response_part)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z59jheWrt9CK"
      },
      "source": [
        "### Universal Translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4-WtLwkt9CL"
      },
      "outputs": [],
      "source": [
        "instruction = \"\"\"Chỉ được trả ra đầu ra câu hỏi và không có giải thích gì. Dịch câu sau từ tiếng Anh sang tiếng Việt?\"\"\"\n",
        "\n",
        "input_data = \"\"\"\n",
        "The new smartphone model offers excellent performance, but its battery life could be better.\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        instruction,  # instruction\n",
        "        input_data,   # input\n",
        "        \"\"            # output\n",
        "    )\n",
        "], return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=128,temperature=0.1, top_p=0.1, use_cache=True)\n",
        "html_output = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
        "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
        "print(response_part)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQvdGE7gt9CL"
      },
      "source": [
        "### Infering logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGbYkdFgt9CL"
      },
      "outputs": [],
      "source": [
        "instruction = \"\"\"Suy luận câu trả lời từ câu chuyện này?\"\"\"\n",
        "\n",
        "input_data = \"\"\"\n",
        "### Câu chuyện:\n",
        "Người trong lớp này là trai hoặc gái. Mọi cậu bé chỉ chơi trò chơi điện tử và các cô gái không chơi trò chơi điện tử. Những người chơi trò chơi điện tử phải có máy tính xách tay chơi game.\n",
        "### Câu hỏi:\n",
        "Một người trong lớp này có máy tính xách tay chơi game. Người này là trai hay gái?\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        instruction,  # instruction\n",
        "        input_data,   # input\n",
        "        \"\"            # output\n",
        "    )\n",
        "], return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=128,temperature=0.1, top_p=0.1, use_cache=True)\n",
        "html_output = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "response_part = html_output.split(\"### Response:\")[1].strip() if \"### Response:\" in html_output else html_output\n",
        "response_part = response_part.replace(\"\\\\n\", \"\\n\")\n",
        "print(response_part)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlhLgHS-t9CM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYFQBJ-bt9CM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wuy55ugFt9CM"
      },
      "outputs": [],
      "source": [
        "import huggingface_hub\n",
        "huggingface_hub.login(\"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\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DO6_5Ls2t9CM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}